# Backend/metrics.py
import math
import nltk
try:
    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
    nltk_available = True
except ImportError:
    nltk_available = False

import re
import numpy as np
from collections import Counter

# Try to import textstat for readability metrics
try:
    import textstat
    textstat_available = True
except ImportError:
    textstat_available = False

# --- Utilities -------------------------------------------------------------
def _ensure_nltk_tokenizers():
    """Ensure required NLTK tokenizer resources are available.

    Handles both legacy 'punkt' and newer 'punkt_tab' resources introduced in
    recent NLTK versions. Downloads quietly if missing. Any download errors are
    swallowed so the caller can decide on fallbacks.
    """
    if not nltk_available:
        return
    # Ensure 'punkt'
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        try:
            nltk.download('punkt', quiet=True)
        except Exception:
            pass
    # Ensure 'punkt_tab' (present on newer NLTK)
    try:
        nltk.data.find('tokenizers/punkt_tab')
    except LookupError:
        try:
            nltk.download('punkt_tab', quiet=True)
        except Exception:
            # Some NLTK versions don't have 'punkt_tab'; ignore
            pass

def calculate_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)):
    """
    Calculate BLEU score between reference and candidate texts.
    
    Args:
        reference: Reference text (ground truth)
        candidate: Candidate text (generated by model)
        weights: Weights for different n-grams. Default is equal weights for 1-4 grams.
        
    Returns:
        BLEU score (float between 0-1)
    """
    if not nltk_available:
        return {"error": "NLTK package not available. Install using 'pip install nltk'."}
    
    try:
        # Ensure we have text to work with
        if not reference or not candidate:
            return {"error": "Reference or candidate text is empty"}

        # Ensure necessary tokenizers are present
        _ensure_nltk_tokenizers()

        # Tokenize the reference and candidate texts
        try:
            reference_tokens = nltk.word_tokenize(reference.lower())
            candidate_tokens = nltk.word_tokenize(candidate.lower())
        except LookupError:
            # Fallback to a simple regex/whitespace tokenization
            reference_tokens = re.findall(r"\w+|[^\w\s]", reference.lower())
            candidate_tokens = re.findall(r"\w+|[^\w\s]", candidate.lower())
        
        # Check if we have tokens
        if not reference_tokens or not candidate_tokens:
            return {"error": "No tokens found in reference or candidate text"}
            
        # Calculate BLEU score with smoothing
        smoothing = SmoothingFunction().method1
        
        # Calculate individual n-gram scores
        bleu_1 = sentence_bleu([reference_tokens], candidate_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothing)
        bleu_2 = sentence_bleu([reference_tokens], candidate_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)
        bleu_3 = sentence_bleu([reference_tokens], candidate_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing)
        bleu_4 = sentence_bleu([reference_tokens], candidate_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)
        
        # Calculate cumulative BLEU score with original weights
        score = sentence_bleu([reference_tokens], candidate_tokens, weights=weights, smoothing_function=smoothing)
        
        return {
            "bleu": score,
            "bleu_1": bleu_1,
            "bleu_2": bleu_2,
            "bleu_3": bleu_3,
            "bleu_4": bleu_4,
        }
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        return {"error": f"Error calculating BLEU score: {str(e)}\nDetails: {error_details}"}

def calculate_perplexity(text, ngram=2):
    """
    Calculate perplexity of the text using n-gram language model.
    
    Args:
        text: Input text
        ngram: N-gram size to use (default: 2)
        
    Returns:
        Perplexity score
    """
    try:
        # Ensure tokenizers are present
        _ensure_nltk_tokenizers()

        # Tokenize the text and prepare n-grams
        try:
            tokens = nltk.word_tokenize(text.lower())
        except LookupError:
            tokens = re.findall(r"\w+|[^\w\s]", text.lower())
        
        # Count n-grams and (n-1)-grams
        ngram_counts = Counter()
        n_minus_1_gram_counts = Counter()
        
        for i in range(len(tokens) - ngram + 1):
            ngram_counts[tuple(tokens[i:i+ngram])] += 1
            n_minus_1_gram_counts[tuple(tokens[i:i+ngram-1])] += 1
        
        # Calculate log probability
        log_prob = 0
        for i in range(len(tokens) - ngram + 1):
            ngram_tuple = tuple(tokens[i:i+ngram])
            n_minus_1_gram_tuple = tuple(tokens[i:i+ngram-1])
            
            # Use add-one smoothing
            prob = (ngram_counts[ngram_tuple] + 1) / (n_minus_1_gram_counts[n_minus_1_gram_tuple] + len(ngram_counts))
            log_prob += math.log(prob, 2)
        
        # Calculate perplexity
        perplexity = 2 ** (-log_prob / (len(tokens) - ngram + 1))
        
        return {"perplexity": perplexity, "ngram": ngram}
    except Exception as e:
        return {"error": f"Error calculating perplexity: {str(e)}"}

def calculate_readability_metrics(text):
    """
    Calculate various readability metrics for the text.
    
    Args:
        text: Input text
        
    Returns:
        Dictionary of readability metrics
    """
    if not textstat_available:
        return {"error": "textstat package not available. Install using 'pip install textstat'."}
    
    try:
        # Calculate various readability metrics
        metrics = {
            "flesch_reading_ease": textstat.flesch_reading_ease(text),
            "flesch_kincaid_grade": textstat.flesch_kincaid_grade(text),
            "gunning_fog": textstat.gunning_fog(text),
            "smog_index": textstat.smog_index(text),
            "automated_readability_index": textstat.automated_readability_index(text),
            "coleman_liau_index": textstat.coleman_liau_index(text),
            "linsear_write_formula": textstat.linsear_write_formula(text),
            "dale_chall_readability_score": textstat.dale_chall_readability_score(text),
            "text_standard": textstat.text_standard(text, float_output=False)
        }
        
        return metrics
    except Exception as e:
        return {"error": f"Error calculating readability metrics: {str(e)}"}

def calculate_readability_delta(original_text, summary_text):
    """
    Calculate the delta between readability metrics of original text and summary.
    
    Args:
        original_text: Original text
        summary_text: Summary text
        
    Returns:
        Dictionary of readability metric deltas
    """
    if not textstat_available:
        return {"error": "textstat package not available. Install using 'pip install textstat'."}
    
    try:
        # Get readability metrics for original and summary
        original_metrics = calculate_readability_metrics(original_text)
        summary_metrics = calculate_readability_metrics(summary_text)
        
        # Check if there were any errors
        if "error" in original_metrics or "error" in summary_metrics:
            return {"error": "Error calculating readability metrics for comparison"}
        
        # Calculate deltas
        deltas = {}
        for key in original_metrics:
            if key == "text_standard":
                continue  # Skip text_standard since it's a string
            
            deltas[key] = summary_metrics[key] - original_metrics[key]
        
        return {
            "original": original_metrics,
            "summary": summary_metrics,
            "delta": deltas
        }
    except Exception as e:
        return {"error": f"Error calculating readability delta: {str(e)}"}

def calculate_all_metrics(reference_text, candidate_text, original_text=None):
    """
    Calculate all available metrics in one call.
    
    Args:
        reference_text: Reference summary
        candidate_text: Generated summary
        original_text: Original text (optional, for readability delta)
        
    Returns:
        Dictionary of all metrics
    """
    results = {}
    
    # Calculate BLEU
    bleu_results = calculate_bleu(reference_text, candidate_text)
    if "error" not in bleu_results:
        results["bleu"] = bleu_results
    else:
        results["bleu"] = {"error": bleu_results["error"]}
    
    # Calculate perplexity for candidate and reference
    cand_ppl = calculate_perplexity(candidate_text)
    ref_ppl = calculate_perplexity(reference_text)
    # Backward-compatible key mirrors candidate perplexity
    results["perplexity"] = cand_ppl if "error" not in cand_ppl else {"error": cand_ppl.get("error", "Unknown error")}
    # Expanded fields
    results["perplexity_candidate"] = cand_ppl
    results["perplexity_reference"] = ref_ppl
    
    # Calculate readability metrics
    if original_text:
        readability_results = calculate_readability_delta(original_text, candidate_text)
        if "error" not in readability_results:
            results["readability"] = readability_results
        else:
            results["readability"] = {"error": readability_results["error"]}
    
    # Also compute readability delta between reference and candidate (if textstat available)
    if textstat_available:
        try:
            ref_vs_cand = calculate_readability_delta(reference_text, candidate_text)
            results["readability_ref_candidate"] = ref_vs_cand
        except Exception as _:
            results["readability_ref_candidate"] = {"error": "Failed to compute reference vs candidate readability"}
    else:
        results["readability_ref_candidate"] = {"error": "textstat package not available. Install using 'pip install textstat'."}
    
    return results